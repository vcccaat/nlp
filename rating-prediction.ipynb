{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare continuous features...\n",
      "Prepare deep features...\n",
      "Prepare wide features...\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 28s 281us/step - loss: 1.1053\n",
      "TRAIN RMSE:  1.0241602816149316\n",
      "VALID RMSE:  1.031410890952519\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "import random\n",
    "from keras.layers import Concatenate, Dense, Dot, Dropout, Embedding, Input, Reshape, LeakyReLU, Add\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow\n",
    "\n",
    "\n",
    "random.seed(2019)\n",
    "np.random.seed(2019)\n",
    "tensorflow.set_random_seed(2019)\n",
    "\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(pred, actual))\n",
    "\n",
    "\n",
    "def build_deepwide_model(len_continuous, deep_vocab_lens, len_wide, embed_size):\n",
    "    input_list = []\n",
    "    continuous_input = Input(shape=(len_continuous,), dtype='float32', name='continuous_input')\n",
    "    input_list.append(continuous_input)\n",
    "\n",
    "    emb_list = []\n",
    "    for vocab_size in deep_vocab_lens:\n",
    "        _input = Input(shape=(1,), dtype='int32')\n",
    "        input_list.append(_input)\n",
    "        _emb = Embedding(output_dim=embed_size, input_dim=vocab_size, input_length=1)(_input)\n",
    "        _emb = Reshape((embed_size,))(_emb)\n",
    "        emb_list.append(_emb)\n",
    "    \n",
    "    wide_input = Input(shape=(len_wide,), dtype='float32')\n",
    "    input_list.append(wide_input)\n",
    "    \n",
    "    deep_input = Concatenate()(emb_list + [continuous_input])\n",
    "    \n",
    "    \n",
    "    dense_1 = Dense(256, activation='linear')(deep_input)\n",
    "    dense_1 = LeakyReLU(alpha=0.001)(dense_1)\n",
    "    #dense_1_dp = Dropout(0.01)(dense_1)\n",
    "    \n",
    "    dense_2 = Dense(128, activation='linear')(dense_1)\n",
    "    dense_2 = Concatenate()([dense_2, dense_1])\n",
    "    dense_2 = LeakyReLU(alpha=0.001)(dense_2)\n",
    "    \n",
    "    #dense_2_dp = Dropout(0.01)(dense_2)\n",
    "    dense_3 = Dense(64, activation='linear')(dense_2)\n",
    "    #dense_3_dp = Dropout(0.01)(dense_3)\n",
    "    dense_2 = Concatenate()([dense_3, dense_2, dense_1])\n",
    "    dense_3_dp = LeakyReLU(alpha=0.001)(dense_3)\n",
    "\n",
    "    \n",
    "    fc_input = Concatenate()([dense_1, dense_2, dense_3_dp, wide_input])\n",
    "    \n",
    "    fc_input = LeakyReLU(alpha=0.001)(fc_input)\n",
    "    model_output = Dense(1)(fc_input)\n",
    "    model = Model(inputs=input_list,\n",
    "                  outputs=model_output)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_continuous_features(df, continuous_columns):\n",
    "    continuous_features = df[continuous_columns].values\n",
    "    return continuous_features\n",
    "\n",
    "\n",
    "def get_top_k_p_combinations(df, comb_p, topk, output_freq=False):\n",
    "    def get_category_combinations(categories_str, comb_p=2):\n",
    "        categories = categories_str.split(', ')\n",
    "        return list(combinations(categories, comb_p))\n",
    "    all_categories_p_combos = df[\"item_categories\"].apply(\n",
    "        lambda x: get_category_combinations(x, comb_p)).values.tolist()\n",
    "    all_categories_p_combos = [tuple(t) for item in all_categories_p_combos for t in item]\n",
    "    tmp = dict(Counter(all_categories_p_combos))\n",
    "    sorted_categories_combinations = list(sorted(tmp.items(), key=lambda x: x[1], reverse=True))\n",
    "    if output_freq:\n",
    "        return sorted_categories_combinations[:topk]\n",
    "    else:\n",
    "        return [t[0] for t in sorted_categories_combinations[:topk]]\n",
    "\n",
    "\n",
    "def get_wide_features(df):\n",
    "    def categories_to_binary_output(categories):\n",
    "        binary_output = [0 for _ in range(len(selected_categories_to_idx))]\n",
    "        for category in categories.split(', '):\n",
    "            if category in selected_categories_to_idx:\n",
    "                binary_output[selected_categories_to_idx[category]] = 1\n",
    "            else:\n",
    "                binary_output[0] = 1\n",
    "        return binary_output\n",
    "    def categories_cross_transformation(categories):\n",
    "        current_category_set = set(categories.split(', '))\n",
    "        corss_transform_output = [0 for _ in range(len(top_combinations))]\n",
    "        for k, comb_k in enumerate(top_combinations):\n",
    "            if len(current_category_set & comb_k) == len(comb_k):\n",
    "                corss_transform_output[k] = 1\n",
    "            else:\n",
    "                corss_transform_output[k] = 0\n",
    "        return corss_transform_output\n",
    "\n",
    "    category_binary_features = np.array(df.item_categories.apply(\n",
    "        lambda x: categories_to_binary_output(x)).values.tolist())\n",
    "    category_corss_transform_features = np.array(df.item_categories.apply(\n",
    "        lambda x: categories_cross_transformation(x)).values.tolist())\n",
    "    return np.concatenate((category_binary_features, category_corss_transform_features), axis=1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tr_df = pd.read_csv(\"data/train.csv\")\n",
    "    val_df = pd.read_csv(\"data/valid.csv\")\n",
    "    te_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "    tr_ratings = tr_df.stars.values\n",
    "    val_ratings = val_df.stars.values\n",
    "\n",
    "    user_df = pd.read_json(\"data/user.json\")\n",
    "    item_df = pd.read_json(\"data/business.json\")\n",
    "    user_df = user_df.rename(index=str, columns={t: 'user_' + t for t in user_df.columns if t != 'user_id'})\n",
    "    item_df = item_df.rename(index=str, columns={t: 'item_' + t for t in item_df.columns if t != 'business_id'})\n",
    "\n",
    "    tr_df[\"index\"] = tr_df.index\n",
    "    val_df[\"index\"]  = val_df.index\n",
    "    te_df[\"index\"] = te_df.index\n",
    "    tr_df = pd.merge(pd.merge(tr_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n",
    "    val_df = pd.merge(pd.merge(val_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n",
    "    te_df = pd.merge(pd.merge(te_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n",
    "\n",
    "    # Continuous features\n",
    "    print(\"Prepare continuous features...\")\n",
    "    continuous_columns = [\"user_average_stars\",\n",
    "#                           \"user_cool\", \n",
    "#                           \"user_fans\", \n",
    "                          \"user_review_count\", \"user_useful\",\n",
    "#                           \"user_funny\",\n",
    "#                           \"item_is_open\", \"item_latitude\", \"item_longitude\", \n",
    "                          \"item_review_count\", \"item_stars\"]\n",
    "\n",
    "    tr_continuous_features = get_continuous_features(tr_df, continuous_columns)\n",
    "    val_continuous_features = get_continuous_features(val_df, continuous_columns)\n",
    "    te_continuous_features = get_continuous_features(te_df, continuous_columns)\n",
    "    scaler = StandardScaler().fit(tr_continuous_features)\n",
    "    tr_continuous_features = scaler.transform(tr_continuous_features)\n",
    "    val_continuous_features = scaler.transform(val_continuous_features)\n",
    "    te_continuous_features = scaler.transform(te_continuous_features)\n",
    "\n",
    "    # Deep features\n",
    "    print(\"Prepare deep features...\")\n",
    "    item_deep_columns = [\"item_city\", \"item_postal_code\", \"item_state\"]\n",
    "    item_deep_vocab_lens = []\n",
    "    for col_name in item_deep_columns:\n",
    "        tmp = item_df[col_name].unique()\n",
    "        vocab = dict(zip(tmp, range(1, len(tmp) + 1)))\n",
    "        item_deep_vocab_lens.append(len(vocab) + 1)\n",
    "        item_df[col_name + \"_idx\"] = item_df[col_name].apply(lambda x: vocab[x] if x in vocab else 0)\n",
    "    item_deep_idx_columns = [t + \"_idx\" for t in item_deep_columns]\n",
    "    item_to_deep_features = dict(zip(item_df.business_id.values, item_df[item_deep_idx_columns].values.tolist()))\n",
    "    tr_deep_features = np.array(tr_df.business_id.apply(lambda x: item_to_deep_features[x]).values.tolist())\n",
    "    val_deep_features = np.array(val_df.business_id.apply(lambda x: item_to_deep_features[x]).values.tolist())\n",
    "    te_deep_features = np.array(te_df.business_id.apply(lambda x: item_to_deep_features[x]).values.tolist())\n",
    "\n",
    "    # Wide (Category) features\n",
    "    \n",
    "    # How to use attribute column for item\n",
    "    print(\"Prepare wide features...\")\n",
    "    #   Prepare binary encoding for each selected categories\n",
    "    all_categories = [category for category_list in item_df.item_categories.values for category in category_list.split(\", \")]\n",
    "    category_sorted = sorted(Counter(all_categories).items(), key=lambda x: x[1], reverse=True)\n",
    "    selected_categories = [t[0] for t in category_sorted[:900]]\n",
    "    selected_categories_to_idx = dict(zip(selected_categories, range(1, len(selected_categories) + 1)))\n",
    "    selected_categories_to_idx['unk'] = 0\n",
    "    idx_to_selected_categories = {val: key for key, val in selected_categories_to_idx.items()}\n",
    "    #   Prepare Cross transformation for each categories\n",
    "    top_combinations = []\n",
    "    top_combinations += get_top_k_p_combinations(tr_df, 2, 200, output_freq=False)\n",
    "    top_combinations += get_top_k_p_combinations(tr_df, 3, 30, output_freq=False)\n",
    "    top_combinations += get_top_k_p_combinations(tr_df, 4, 20, output_freq=False)\n",
    "    top_combinations += get_top_k_p_combinations(tr_df, 5, 10, output_freq=False)\n",
    "    \n",
    "    top_combinations = [set(t) for t in top_combinations]\n",
    "\n",
    "    tr_wide_features = get_wide_features(tr_df)\n",
    "    val_wide_features = get_wide_features(val_df)\n",
    "    te_wide_features = get_wide_features(te_df)\n",
    "\n",
    "    # Build input\n",
    "    tr_features = []\n",
    "    tr_features.append(tr_continuous_features.tolist())\n",
    "    tr_features += [tr_deep_features[:,i].tolist() for i in range(len(tr_deep_features[0]))]\n",
    "    tr_features.append(tr_wide_features.tolist())\n",
    "    val_features = []\n",
    "    val_features.append(val_continuous_features.tolist())\n",
    "    val_features += [val_deep_features[:,i].tolist() for i in range(len(val_deep_features[0]))]\n",
    "    val_features.append(val_wide_features.tolist())\n",
    "    te_features = []\n",
    "    te_features.append(te_continuous_features.tolist())\n",
    "    te_features += [te_deep_features[:,i].tolist() for i in range(len(te_deep_features[0]))]\n",
    "    te_features.append(te_wide_features.tolist())\n",
    "\n",
    "    # Model training\n",
    "    deepwide_model = build_deepwide_model(\n",
    "        len(tr_continuous_features[0]),\n",
    "        item_deep_vocab_lens,  \n",
    "        len(tr_wide_features[0]), \n",
    "        embed_size=100)\n",
    "    deepwide_model.compile(optimizer='adagrad', loss='mse')\n",
    "    history = deepwide_model.fit(\n",
    "        tr_features, \n",
    "        tr_ratings, \n",
    "        epochs=1, verbose=1, callbacks=[ModelCheckpoint('model.h5')])\n",
    "\n",
    "    # Make Prediction\n",
    "    y_pred = deepwide_model.predict(tr_features)\n",
    "    print(\"TRAIN RMSE: \", rmse(y_pred, tr_ratings))\n",
    "    y_pred = deepwide_model.predict(val_features)\n",
    "    print(\"VALID RMSE: \", rmse(y_pred, val_ratings))\n",
    "#     y_pred = deepwide_model.predict(te_features)\n",
    "#     res_df = pd.DataFrame()\n",
    "#     res_df['pred'] = y_pred[:, 0]\n",
    "#     res_df.to_csv(\"{}.csv\".format(STUDENT_ID), index=False)\n",
    "#     print(\"Writing test predictions to file done.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
